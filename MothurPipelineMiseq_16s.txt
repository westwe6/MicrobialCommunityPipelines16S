#Open your working directory, which contain the files you want to work with (fastq)
cd /Users/MacbookPro/Desktop/Mothur/MiSeq_SOP

#Open the program mothur (find where it is located on your computer)
/Users/MacbookPro/Desktop/Mothur/mothur/mothur


#Trim and QC sequences, remove barcodes, and primer dimers (need to make into a bash shell script .sh)

#!/bin/bash
	
	for file in $(<filelist_PE_QC.txt)
do
	java -jar $TRIM/trimmomatic  PE -threads 4 -trimlog ${file}_QC_PE_log.fasta ${file}_L001_R1_001.fastq ${file}_L001_R2_001.fastq ${file}_L001_R1_001_pTrim.fastq ${file}_L001_R1_001_uTrim.fastq ${file}_L001_R2_001_pTrim.fastq ${file}_L001_R2_001_uTrim.fastq ILLUMINACLIP:AdapterRef16S.fa:2:30:12 LEADING:20 TRAILING:20 MINLEN:125

done

#May need to use fastq-quality-filter to remove any low quality basepairs/reads
#Fastq_quality_filter
#fastq_quality_filter -v -q 20 -p 50 -i IA_7_T_16S_CGAAGTAT-CTACTATA_L001_R2_001.fastq -o IA_7_T_16S_CGAAGTAT-CTACTATA_L001_R2_001_qc.fastq

#	version 0.0.6
#	   [-h]         = This helpful help screen.
#	   [-q N]       = Minimum quality score to keep.
#	   [-p N]       = Minimum percent of bases that must have [-q] quality.
#	   [-z]         = Compress output with GZIP.
#	   [-i INFILE]  = FASTA/Q input file. default is STDIN.
3	   [-o OUTFILE] = FASTA/Q output file. default is STDOUT.
#	   [-v]         = Verbose - report number of sequences.
#			  If [-o] is specified,  report will be printed to STDOUT.
#			  If [-o] is not specified (and output goes to STDOUT),
#			  report will be printed to STDERR.


#Make contigs (this can be slow, depending on the # of processors chosen)
#Contigs are a contiguous sequence of DNA created by assembling overlapping DNA fragments.
make.contigs(file=stability.files, processors=20)

#Let's look at a summary of the files contigs you just made (ambiguities, contig lengths, etc)
summary.seqs(fasta=stability.trim.contigs.fasta)

#Removes sequences with greater than 275bp (this eliminates sequences w/ ambiguities or sequences longer than your target sequence length)
#Use either of the following 2 lines of code (prefer the second as it creates three files)
screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275)
screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, summary=stability.trim.contigs.summary, maxambig=0, maxlength=275)

#This helps you keep track of what files mothur is using in the current session
get.current()

#removes duplicate sequences
unique.seqs(fasta=stability.trim.contigs.good.fasta)

#generates a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group.
count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups)

#creates a file called stability.trim.contigs.good.count_table
summary.seqs(count=stability.trim.contigs.good.count_table)

#makes a reference sequence customized to the start and end of the sequence (important:you need to know the length and region of this reference sequence)
pcr.seqs(fasta=silva.bacteria.fasta, start=11894, end=25319, keepdots=F, processors=20)

#summarizes newly created silva.v4.fasta file (contains information about sequence identity, start and end)
summary.seqs(fasta=silva.v4.fasta)

#Aligns your contigs to the reference sequences (creates a report file and a file containing the alignments)
align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.v4.fasta, flip=true)

#Summary of the contig-reference alignment (similar to previous summary, just lets you know you have data in the file)
summary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table)

#So by looking at the previous summary you will see that some sequences start and end in the same spot, this could be due to deletions or insertions and so we have to remove sequences that don't fit
screen.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table, summary=stability.trim.contigs.good.unique.summary, start=1968, end=11550, maxhomop=6)

#Now we should have sequences that are aligned, but there are still overhangs
summary.seqs(fasta=current, count=current)

#Let's truncate the overhangs of each sequences (all sequences after this step will have the same length and start end in the same place). We now have a 376bp sequence and removed 13049 blanks at end
filter.seqs(fasta=stability.trim.contigs.good.unique.good.align, vertical=T, trump=.)

#an easier way to look at summary files (use the current files)
summary.seqs(fasta=current, count=current)

# Now let's remove any duplicate sequences present again, since the truncation likely created a bunch of duplicates
unique.seqs(fasta=stability.trim.contigs.good.unique.good.filter.fasta, count=stability.trim.contigs.good.good.count_table)

#an easier way to look at summary files (use the current files)
summary.seqs(fasta=current, count=current)

#Let's cluster sequences by how many differences they have (rule of thumb 1nt difference per 100bp). This will combine sequences if nucleotide difference is less than  or equal to 2.
pre.cluster(fasta=stability.trim.contigs.good.unique.good.filter.unique.fasta, count=stability.trim.contigs.good.unique.good.filter.count_table, diffs=2)

#Removes chimeras from count table (default: dereplicate=F, which removes chimeras automatically, be careful it sometimes removes rare sequences)
chimera.uchime(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.count_table, dereplicate=t)

#remove chimeras from fasta file
remove.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.accnos)

#an easier way to look at summary files (use the current files) (removed 7.3% of sequences, found chimeric should be less than 10%)
summary.seqs(fasta=current, count=current)

#Bayesian classifier (classifies sequences based on known DNA samples and we want to remove unknown or randomly amplified "stuff", this is very specific to your study)
classify.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, reference=trainset9_032012.pds.fasta, taxonomy=trainset9_032012.pds.tax, cutoff=80)
remove.lineage(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.taxonomy, taxon=Chloroplast-Mitochondria-Eukaryota)

#2589 unique sequences and a total of 119301 total sequences and 350 of the sequences were in various groups (see above groups)
summary.seqs(fasta=current, count=current)


#Check error rate (avoid this right now, we could do it in R if need be)
get.groups(count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, groups=Mock)
seq.error(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta, reference=HMP_MOCK.v35.fasta, aligned=F)


#Clustering and rarefaction
dist.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta, cutoff=0.20)
cluster(column=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.dist, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table)
make.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table, label=0.03)
rarefaction.single(shared=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.an.unique_list.shared)

remove.groups(count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.taxonomy, groups=Mock)


#Now we have a couple of options for clustering sequences into OTUs. For a small dataset like this, we can do the traditional approach using dist.seqs and cluster:
#dist.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta, cutoff=0.20)
#cluster(column=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.dist, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.uchime.pick.pick.pick.count_table)

#The alternative is to use our cluster.split command. In this approach, we use the taxonomic information to split the sequences into bins and then cluster within each bin. We've published results showing that if you split at the level of Order or Family, and cluster to a 0.03 cutoff, you'll get just as good of clustering as you would with the "traditional" approach. The advantage of the cluster.split approach is that it should be faster, use less memory, and can be run on multiple processors. In an ideal world we would prefer the traditional route because "Trad is rad", but we also think that kind of humor is funny.... In this command we use taxlevel=4, which corresponds to the level of Order.
cluster.split(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.pick.taxonomy, splitmethod=classify, taxlevel=4, cutoff=0.15)

#Next we want to know how many sequences are in each OTU from each group and we can do this using the make.shared command. Here we tell mothur that we're really only interested in the 0.03 cutoff level:
make.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table, label=0.03)

#We probably also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the classify.otu command:
classify.otu(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.pick.taxonomy, label=0.03)

phylotype(taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.pick.taxonomy)

make.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.pick.tx.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table, label=1)

classify.otu(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.pick.tx.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pds.wang.pick.pick.taxonomy, label=1)
